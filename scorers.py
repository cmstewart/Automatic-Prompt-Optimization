from __future__ import annotations
import hashlib
import threading
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple

import numpy as np
import tensorflow_hub as hub
import tensorflow as tf
from transformers import AutoTokenizer


# OLD BEM LOADER
#BEM_URL: str = ("https://kaggle.com/models/google/bert/frameworks/TensorFlow2/variations/answer-equivalence-bem/versions/1/model")
#BEM_MODEL = hub.load(BEM_URL)
import os, tensorflow as tf, kagglehub
from functools import lru_cache

BEM_TAG = "google/bert/tensorFlow2/answer-equivalence-bem"

@lru_cache(maxsize=1)
def load_bem():
    base_dir = kagglehub.model_download(BEM_TAG)

    #  first try the cache root
    root_pb = os.path.join(base_dir, "saved_model.pb")
    if os.path.exists(root_pb):
        return tf.saved_model.load(base_dir)

    # otherwise fall back to a nested “model/” folder (old layout)
    nested_dir = os.path.join(base_dir, "model")
    if os.path.exists(os.path.join(nested_dir, "saved_model.pb")):
        return tf.saved_model.load(nested_dir)

    raise FileNotFoundError(
        "Could not locate saved_model.pb in KaggleHub BEM download."
    )

BEM_MODEL = load_bem()
_BEM_LOCK  = threading.Lock()
BEM_tokenizer  = AutoTokenizer.from_pretrained("bert-base-uncased")
BEM_threshold   = 0.56



def predict_on_example(inputs):
    """
    Worker helper for concurrent evaluation.
    Returns: (prompt_text, example_dict, LLM_prediction_string)
    """
    ex, predictor, prompt = inputs
    pred = predictor.inference(ex, prompt)
    return prompt, ex, pred



class BaseScorer(ABC):
    """
    Implements a prompt-hash and example-id, prediction cache so the
    optimiser never calls the LLM twice for the same pair.
    """

    def __init__(self, predictor):
        self.predictor = predictor
        self._pred_cache: Dict[Tuple[str, int], str] = {}
        self._cache_lock = threading.Lock()

    @staticmethod
    def _hash_prompt(prompt: str) -> str:
        #return hashlib.md5(prompt.encode()).hexdigest()[:12]
        return hashlib.md5(prompt.encode("utf-8")).hexdigest()

    # scorers.py  –  replace the whole _predict method
    def _predict(self, examples: List[Dict], prompt_text: str) -> List[str]:
        ph = self._hash_prompt(prompt_text)
        missing = []
        # 1) identify which examples still need a call
        for ex in examples:
            key = (ph, ex["id"])
            if key not in self._pred_cache:
                missing.append(ex)

        # 2) fetch only the missing ones
        if missing:
            new_preds = self.predictor.batch_inference(missing, prompt_text)
            with self._cache_lock:
                for ex, p in zip(missing, new_preds):
                    self._pred_cache[(ph, ex["id"])] = p

        # 3) now build the prediction list in the original order
        return [self._pred_cache[(ph, ex["id"])] for ex in examples]


    @abstractmethod
    def __call__(self, examples: List[Dict], prompt_text: str) -> float:
        ...



class BEMScorer(BaseScorer):
    
    # pair_prob returns float in [0,1]
    # pair_equivalent returnsboolean using self.tau
    # __call__  returns mean boolean accuracy over the minibatch
    

    def __init__(self, predictor, bem_threshold: float = BEM_threshold):
        super().__init__(predictor)
        self.tau = bem_threshold
        global BEM_MODEL
        

    def pair_prob(self, pred: str, gold: str, question: str) -> float:
        encoding = BEM_tokenizer(pred, gold, truncation=True, max_length=512, padding="max_length", return_tensors="pt")
        inputs = {
            "input_ids":tf.cast(encoding["input_ids"], tf.int64),
            "segment_ids": tf.cast(encoding["token_type_ids"], tf.int64),
        }
        with _BEM_LOCK:
            logits = BEM_MODEL(inputs)
             # If the SavedModel ever returns a dict on another platform, keep the following safeguard:
        if isinstance(logits, dict):           
            logits = list(logits.values())[0]
        prob_equiv = tf.nn.softmax(logits, axis=-1).numpy().squeeze()[1]
        return float(prob_equiv)
    
    # scorers.py
    def batch_pair_prob(self, preds: list[str], golds: list[str], questions: list[str]) -> list[float]:
        """
        Vectorized BEM: one SavedModel call for the whole mini-batch.
        IMPORTANT: the SavedModel expects shape (None, 512) for both inputs.
        """


        # 1) tokenize with fixed length 512
        enc = BEM_tokenizer(
            preds,
            golds,
            truncation=True,
            max_length=512,
            padding="max_length",          # ← pad to 512, not to longest
            return_tensors="pt",
        )

        # Some tokenizers may omit token_type_ids; ensure it exists
        if "token_type_ids" not in enc:
            # make a zeros segment_ids tensor with the exact same shape
            seg = tf.zeros_like(enc["input_ids"], dtype=tf.int64)
        else:
            seg = tf.cast(enc["token_type_ids"], tf.int64)

        # 2) cast to int64 and enforce exact (B, 512) shapes
        input_ids = tf.cast(enc["input_ids"], tf.int64)
        # Optional (helps with static tracing)
        input_ids = tf.ensure_shape(input_ids, [None, 512])
        seg       = tf.ensure_shape(seg,       [None, 512])

        inputs = {"input_ids": input_ids, "segment_ids": seg}

        # 3) forward
        with _BEM_LOCK:
            logits = BEM_MODEL(inputs)
            # Some TF SavedModels return dicts; normalize to a tensor
            if isinstance(logits, dict):
                # take the first (only) output
                logits = next(iter(logits.values()))

        # 4) convert to probability that pred ≡ gold (class 1)
        prob_equiv = tf.nn.softmax(logits, axis=-1).numpy()[:, 1]
        return prob_equiv.tolist()




    def pair_equivalent(self, pred: str, gold: str, question: str) -> bool:
        return self.pair_prob(pred, gold, question) >= self.tau

    def __call__(self, examples: List[Dict], prompt_text: str) -> float:
        preds = self._predict(examples, prompt_text)
        hits = [
            self.pair_equivalent(p, ex["answer"], ex["question"])
            for p, ex in zip(preds, examples)
        ]
        return float(np.mean(hits))